# Building a Large Language Model from scratch 

This repository is dedicated to learning how to build a Large Language Model (LLM) from scratch using PyTorch. It is based on the book Building a Large Language Model from Scratch by Sebastian Raschka. The goal is to deeply understand the fundamentals of LLMs, implement them step by step, and create a functional model using PyTorch.

Book Repo:
https://github.com/rasbt/LLMs-from-scratch/

### Objectives
- Understand the core components of LLMs.
- Learn PyTorch fundamentals in the context of LLM development.
- Implement a Transformer-based LLM from scratch.
- Train, evaluate, and fine-tune the model.
- Deploy the trained model using Flask/FastAPI.

### Prerequisites
To follow along, you should have:
- Basic knowledge of Python.
- Familiarity with deep learning concepts.
- Some experience with PyTorch (helpful but not mandatory).
- A GPU-enabled system (recommended for faster training).

### Code Implementation â€“ Step-by-step PyTorch-based implementation of an LLM:
- Tokenization & Data Preprocessing
- Building the Transformer Model from Scratch
- Training Pipeline (Loss Function, Optimizer, etc.)
- Fine-tuning & Evaluation
- Deployment with FastAPI or Flask (if needed)
